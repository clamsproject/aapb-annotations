# AAPB-CLAMS Annotation Repository
This repository contains datasets from manual annotation projects in [AAPB](https://americanarchive.org/) - [CLAMS](https://clams.ai) collaboration.

## Project Information
[GBH](https://www.wgbh.org/) and the American Archive of Public Broadcasting (AAPB) have involved the CLAMS team in a collaboration to develop metadata creation systems for digital archives of public media 
(primarily video and audio from publicly-funded tv shows and radio broadcasts). 
This will facilitate the research and preservation of significant historical content from such media.  
The process of archiving, summarizing and extracting-metadata from this media could eventually be automatic.  
This repository/endeavor provides training and evaluation data for the Computer Vision and Machine Learning tools in this process.  
Data collected is used to evaluate the success of the tools. Then, the tools will be trained on larger batches of data to automatically retrieve 
information important to the archival process.

## Structure of This Repository/Directory
 - `batches` subdirectory
 - project subdirectories
 - this README file

### `batches` Subdirectory 
The first directory is the special `batches` directory. 
This special directory maintains tracking information for the whole repository/annotation effort. 
Annotation projects are done in batches, which denote a moment/event/period of an annotation effort on a set of media assets.
The batches are set of the identifying tags for those respective media assets.
Specifically, this directory contains `.txt` files named after the batch name. Batches are usually named after their relevant GitHub issue from [AAPB-CLAMS collaboration repository](https://github.com/clamsproject/aapb-collaboration).  
* Each line in a `.txt` file _must_ be a single AAPB GUID, with an exception to any lines starts with `#` - which denotes a comment.  
* A GUID is a unique identifying string that can be used at the AAPB website to find one particular media and its supporting files, eg. `cpb-aacip-96d289b264c` at https://americanarchive.org/catalog/cpb-aacip-96d289b264c.

> [!NOTE]
> AAPB-GUID is not [Universally unique identifier](https://en.wikipedia.org/wiki/Universally_unique_identifier), but just a unique identifier within the scope of the AAPB system.   

### Project Subdirectories 
Each directory in this repository represents a specific annotation project, its datasets and processing tools.    
This includes its raw annotated data file, gold-formatted final output data file for tool ingestion, software-suite for converting from raw to gold, and a project-specific `readme.md` explaining it and its annotation guidelines.

The directory name is the name of the project. Each directory contains the following files:

#### Raw intermediate dataset files
> [!IMPORTANT]
> `YYMMDD-batchName` directory 
 
These sub-directories contains raw output files from the manual annotation process created by the annotation tool (or by hand like a `.csv` file). 
Different annotation tools create different file formats with diverse formatting. The `YYMMDD-` prefix _must_ indicate the time when a batch of annotation is conducted. (e.g., when the batch is decided to be annotated)
These prefixes are important for easy sorting of annotation processes and machine ingestion of the raw data. 
The `batchName` part of the directory name _must_ match only one of `.txt` files in the batches. 

#### Codebase for conversion
> [!IMPORTANT]
> (usually) `process.{sh,py}` and dependencies
 
A piece of software to process the raw manual annotations and generate the publicly-available "gold" dataset. 
The input file format to the tools/apps can vary (e.g. `.csv`, `.json`). 
In addition to the main code file, if the code requires additional dependencies/scripts, they can stay in the same directory. 
The dependencies information can be written down in the `README.md` (below) file or in a machine-friendly file with the list of dependencies (e.g. `requirements.txt` for `pip`).

#### Final "Gold" dataset files
> [!IMPORTANT]
> `golds` directory

This directory contains the public "gold" dataset generated by the above script. The script _must_ generate one file per GUID (video/audio/text document) and the number of gold files in this directory _must_ match the sum of GUIDs in all batches (`YYMMDD-xxx` subdirectories) annotated. 
The gold dataset is a set of files that are in a format that is ready for use with the newly developed tools. I.e. The raw file must be properly formatted so that tools in the next step of the process can use this dataset.

#### Information README
> [!IMPORTANT] 
> `README.md` (and possibly `guidelines.md`)

Project-specific information, including but not limited to:
* Annotation project name
* Annotator summary: Some basic demographic information about the annotators. Age group, native languages, language proficiency, occupational characteristics, etc.
* Annotation environment/tool information (name, version, link, tool used, user manual, etc.)
* Project changes: version changes, addition of new batches, change in annotator personnel, etc.
* Raw-to-gold conversion code explanation 
  * dependencies, short description of `process.py`
  * file formats of raw and gold
  * field description, _datatype_
  * differences, added information, discarded information during `process.py`, added info during process.py, etc.)  
* Annotation guidelines - sometimes this is a separate file: `guidelines.md`: How to annotate in this project, aka scheme. This section should give sufficient information for the reproduction of the annotation to produce almost exact similar raw datasets.
  * What tool is used, and how it is used. In most cases there is a separate codebase (ideally on `clamsproject` GitHub) for the annotation tool, and it should be linked in the project-specific `README.md`.
  * What to annotate
  * Options of label choices
  * Label formatting. 
  * Differentiation between labels, edge cases, other decisions made during annotation.
  * Concerns, limitations, precision details. (e.g. Annotation of time is likely only down to 0.100-0.200 seconds of precision.)

> [!NOTE]
> `readme.md` files are supposed to be actively maintained by the project manager. All `guideline.md` files are recommended to be version-controlled.  



## Repository-level Conventions
### Time Point Notation
> `hh:mm:ss.mmm` with a DOT 

The time format for all (gold) datasets in this repository is [ISO 8601 Time Format](https://en.wikipedia.org/wiki/ISO_8601#Times). 
Specifically, with time precision of up to hours and down to 3-digits in milliseconds placed after seconds with a **DOT**, where `00:00:00.000` means the very beginning of a time-based media (not midnight real-time).

This instruction is specifically to be  required for the "gold" data formats throughout this repository, except where exceptions are required and documented.  
(_TODO: Current/Old gold datasets and tools have yet to be converted._)  
During raw annotation however, the tool environments may use different time formats because of non-in-house authors. 
Moving forward, the expectation for any in-house tools and apps are to use this standard. 
If configurable, the annotation tool should be configured to use this format. If possible, the annotator should also be instructed to use this format. 

For MPEG-based video files, frame numbers are converted to milliseconds with loss of precision past 3-digits. 
However, due to exact time -> still image fetching being dependent on the video compression/codec/player, there is no expected need for precision past 3-digits. 
It is assumed that different video players will regenerate images on screen slightly differently based on the decompression algorithms. 
To that end, it is unlikely that even given a specific time moment that a person in one place would be able to extract exactly the same pixels 
in a frame as another person doing it somewhere else.  
This convention was put forth upon consultation of the needs of GBH and CLAMS. 
### Imprecision in Annotation in General
Data Quality processes are currently still being built. Currently, datasets do not have a data quality checklist applied to them. 
This means there are possible typos in anything that must be typed. Other general data messiness is also possible. 
Two semi-preventative measures are: 
1. If only a few value options are expected in one column, using a pie chart/counter in Excel to search/count for typos to manually correct is possible. 
2. Annotators should use copy-paste wherever possible instead of typing, and annotation tools should have buttons to add items, 
reducing typos during typing.  

The current convention is that annotators are asked to be as careful as meaningfully possible, and some datasets are "quality-assumed" upon 
faith in annotators/environment until such time a quantitative analysis of errors is done.  

### Imprecision in Time-based Annotation
Time-based annotations are almost inherently imprecise. This is due usually to either perception or manipulation of the tool within the limits of meaningful
task-speed constraints are at hand. 
Furthermore, the features of audiovisual materials do not always have clear-cut beginning and end points.    
The conventions in place attempt to provide clarity for when generally the annotations can be considered precise or not. 

1. **MARGIN OF ERROR** _(+/- in both directions)_- 
The margin of error depends on the process of how an annotation project was conducted and the tool used. 
For instance, 
if the annotator is playing a video at half speed (audio listenable) and pressing a button when a chyron appears (and not stopping to correct or precisely verify), we can assume:
   1. Precision is limited by on-screen-and-discern-to-press [reaction time](https://www.reddit.com/r/truegaming/comments/hu0p3a/comment/fylge12/?utm_source=reddit&utm_medium=web2x&context=3),
which is approximately .200 to .250 seconds. This time amount is somewhat similarly shown in feedback from musicians using digital keyboards and video gamers complaining about [ping or framerate](https://www.pcgamer.com/how-many-frames-per-second-can-the-human-eye-really-see/).
   2. As an assumption, if the video is playing at half speed, we can then assume the margin of error from this factor is relatively reasonably represented as around 0.100-0.200 seconds. 
   3. Being able to pause and visually move a time slider with high precision would increase the precision but likely lead to excessive annotation labor cost. 
   4. It is highly likely there will be cases where margin of error will pass over the Directionality limit given below. 
However, the convention requested during annotation is to attempt to preserve **Directionality** instructions over reducing **Margin of Error**.   
  
3. **DIRECTIONALITY** _(as close to a certain boundary but not past it)_ - We attempt to give instructions and explanation for when we want 
an annotation to be up to the limit of something as close as possible. E.g. We're annotating a fading-in-and-out slate. 
We want the start time of the slate to be "as close as possible but after" the moment when the slate is fully solid and no longer transparent.
And we want the end time of the slate to be "as close as possible but before" the moment when the slate begins to start fading-out. 
Here are two ways to describe the Directionality convention: 
   1. **Instructions for human annotators** - For time-based media, annotated times are to be labeled as within the duration of the phenomenon.
Eg. if a text-based label fades on-and-off screen, 
the annotated start time should be the moment it has become fully solid and the end time should be the moment just before it begins to fade.  
   2. **For interpreting MMIF/annotations** - 
For time-based media, assume that the interval defined in the MMIF annotation is contained within the relevant interval in the actual media.

Finally, a reminder must be placed that at 30 frames per second, each frame is 0.033_ seconds long, meaning a tenth of a second has 3 frames within it.
Practically speaking, there is only a small percentage of cases where the variation between one frame to its neighbor is relevant,
especially in cases of human perception.
The conventions for precision hold until new needs of the project are required. 
  
## List of Current Projects/Subdirectories
_This section is currently manually updated and may be incomplete. It contains information up to the readme's editing date._  
* (`batches`)
* `january-slates` - slates are actual visible frames within the video media that contain the metadata and other identifying information of that video. 
  * eg. program name, director, producer, etc.
  * Project done in January. This is an outdated naming convention.
* `newshour-chyron` - drawn from the [NewsHour](https://americanarchive.org/special_collections/newshour) TV broadcast, 
this project annotates text appearing on screen, usually above or below the main action saying things such as "Breaking News", "Joan, author".
* `newshour-namedentity` - from NewsHour. This project annotated [named entities](https://www.techtarget.com/searchbusinessanalytics/definition/named-entity#:~:text=In%20data%20mining%2C%20a%20named,phone%20numbers%2C%20companies%20and%20addresses.)
found within the video _transcript_ along with which characters denoted that named entity and its type
 (see `newshour-namedentity/{guidelines,readme}.md`).
* `newshour-namedentity-wikipedialink` - from NewsHour. This project used the previous project's dataset and added
an extra label of which wikimedia link referred to the named entity annotated, eg. https://www.wikidata.org/wiki/Q931148.
* `newshour-transcript` - from NewsHour. This project found the start and end times for 10 tokens of closed captioning at a time from the transcript to the video. 

## Issue Tracking and Conversation Archive
Progress and other discussion by AAPB/CLAMS/WBGH is tracked via the open and closed [Github Issues](https://github.com/clamsproject/aapb-annotations/issues) feature.  
Finally, please email [CLAMS.ai admin](admin@clams.ai) for other inquiries.  