# NewsHour Transcript Sync
## Project Overview
To annotate the start times/end times of a written text transcript of audible speech within videos.  
In other words, providing the timestamps for speech recognition in closed captioning.  

> [!IMPORTANT]  
> The "gold" Newshour transcript text used for this alignment process is proprietary, IP-protected material.
> - **To project managers and developers:** BE CAREFUL NOT TO UPLOAD the raw gold transcript text to public repositories.
> - **To users of the dataset:** The information in this directory might not be complete. The text used in the original `.srt` annotation is not the same as the proprietary gold standard text. Please contact the project managers if you are interested in accessing the full data.

### Specs
* Annotation Project Name - `newshour-transcript-sync`
* Annotator Demographics
    * Number of annotators - 1
    * Occupational description - College student
    * Age range - 20s
    * Education - College
* Annotation Environment Information
    * Name - CADET
    * Version - Unknown
    * Tool documentation - (see below tool installation)
* Project Changes
    * Number of batches - 1
    * Other version control information - None  

## Tool Installation: CADET
CADET - Caption and Description Editing Tool  
[CADET and Installation Instructions](https://www.wgbh.org/foundation/services/ncam/cadet)   
There are three basic ingredients to working with CADET. A video, its transcript, and the application itself.  
You'll have to have downloaded all of these before you can properly begin.  
  
To import the transcript, go to _File_, then _Import..._.  
Doing so will open up a window with the heading **Import Timed-Text File**.  
At the top left, you'll find a dropdown menu labeled _Import Type:_. You'll want to select Plain, otherwise, no transcripts will appear.  
Go to where you have the transcripts downloaded on your computer, and then select the appropriate one.  
  
Importing the video: Go to _File_, then _open Media..._.  
This will open up a window with the heading **Open Media File**.  
Then you will mostly follow the same process as importing a transcript, without the dropdown menu to select _Import Type:_.  
Go to where the videos are downloaded on your computer, then select the video.  
With both video and transcript downloaded, you can now do the closed captioning.  

## Annotation Guidelines
Transcribing/Closed Captioning  
> [!Note]  
> This `readme.md` is the guideline for this project.  
### Preparation
INPUT - Videos with audible speech to be annotated and transcripts matching those videos.  
  
First, break the transcript up into groups of 10 words. For every 10th word spoken, make a new line.  
* If a word is included within the transcript but isn't spoken in the video, delete it and don't count it when making new lines.  
* Closed Captioning exists to relay what is being spoken, so headers and descriptors in brackets aren't needed.  
Headers and descriptors in brackets include script instructions, stage directions, who is speaking, usually denoted by capital text or text in brackets eg.  (`JIM LEHRER`: I am saying something. `[MUSIC PLAYS]`)     
Once the transcript has been split into groups of 10 words, you can begin assigning those words to the appropriate times in the video.  
  
Second, youâ€™re going to want to go to _Player_, then hit _Play slower_ twice. This will play the video at .5x normal speed.  
### What to Annotate
Timestamping
* `text` - the text of the transcription is broken into pieces and entered into the annotation file from doing the above. 

* `start` (time) - When you hear the first word of the first line, hit the control and comma keys at the same time. (Different computers may have different hotkeys.)  
If you go to _Events_, the proper shortcut will be next to the option that says _Insert start time_.  
  
When you hear the first word of the second line, do the same thing, and so on and so forth.  
As you continue to do this, you'll go further down the transcript until you've gone through the whole thing.  
This can be a very long and tedious process, but it demands your complete attention for proper timing.  
That's also why you want the video at .5x normal speed.  

(Annotator:) I would go through videos with one finger over control and the other on comma.
This was probably the most demanding thing that I did during my time at WGBH. If you have any further questions, notify me (Gabriel) on Slack. 

* `end`(time) - end times were automatically generated by the CADET tool as 10ms prior to the next start time. When this project was done, end times were not annotated manually. 
Instead, the decision was made to simply have the annotation of the time duration go until the next start time comes in. It is unvalidated what happened at the end of the transcript. 
### How to Annotate It
(See above)

### Decisions, Differentiation, and Precision Level during Annotation
* **`start` (time)** - In  Nov 01, 2022 (221101) when this first annotation was done, only start times were considered and annotated.  
* **`end` (time)** -  were automatically generated by the CADET tool as 10ms prior to the next start time.  

* **Long Pauses** - This means that if there was a long pause, the duration will continue to span until the next start-of-10-words was annotated.
Eg. in raw file [cpb-aacip-507-1v5bc3tf81.srt](https://github.com/clamsproject/aapb-annotations/blob/main/newshour-transcript-sync/221101-aapb-collaboration-21/cpb-aacip-507-1v5bc3tf81.srt), annotation #6 has a duration of 40s instead of the usual 4-5s.  

* **Time Format Unknown** - The time format in the raw files is likely milliseconds.
  
* **Text Mismatch to tool's given transcript** - As noted above, the transcript was edited and changed to remove non-spoken material. 
A significant discrepancy was discovered between the text used during the original CADET annotation (found in the `.srt` files) and the official "gold" standard Newshour transcripts (the `.txt` files). It looks like the original annotation process involved editing the transcript to remove non-spoken material or started with edited transcript (NOT CONFIRMED), which resulted in a version of the text that is substantially different from the gold standard.
At the same time, it is possible that the text was also transformed by conjoining the spoken lines of different people. 
This issue has been resolved by implementing additional text alignment in `process.py`. For a detailed history of this issue, see [GitHub Issue #5](https://github.com/clamsproject/aapb-annotations/issues/5).
* **Other Difficulties and Interjections** - Closed Captioning/Subtitling of live interviews is difficult due to messy original data such as people speaking over each other. 
Here, "what the best subtitle would be" was used to guide annotator intuition.  
In cases where people spoke over each other (continuing=1st, interjecting=2nd), the given transcript often included what the interjector said after the transcription of what the continuing person said. 
The start time of the interjection is set at the moment when the first word of the interjection text is, and the continuing phrase from the previous line is annotated with an earlier end time (stopping where the interjection occurs).  
There are possible cases of multiple interjections, complete talking-over of people, pure unintelligible garble. The annotation of start times was done to the best of a reasonable ability.  
There is also little that could be done if the transcript did not confirm to the audio, and annotation would have to continue generally as best as possible with the mismatch as correcting it likely led to a slippery slope of revisions and decisions.  
  
* **Margin of Error** - Exact measurements of margin of error of when start times are clocked is not known, nor how much interjection or garble there was.  
However, with the above method of slowing down the audio to 50% speed and careful annotation, general beginning times should be as close as possible.
The datasets in this project are unvalidated for errors and precision. The assumption/hope is that the time error is approximately similar as described in the [repository_level_conventions.md](https://github.com/clamsproject/aapb-annotations/blob/main/repository_level_conventions.md).

## Data Format and `process.py`
### `raw` data
`.srt` file of lines of labels. Info about the [file format](https://mailchimp.com/resources/what-is-an-srt-file/#:~:text=An%20SRT%20file%2C%20or%20SubRip,any%20video%20or%20audio%20content.) here.    
* Fields: 
    * `label_number` - entry number
    * `start` - the starting time of that line of text.
    * `end` - the ending time of that line of text.
    * `text` - the 10 tokens of the closed captioning that are spoken to be timed in the transcript.
* Format:
```
label_number  
start --> end
text
empty_line  
```
* Example:
```
1
00:02:42,210 --> 00:02:46,860
Good evening. Energy Secretary James Schlesinger today uttered the direst

2
00:02:46,870 --> 00:02:50,990
warnings about the economic future unless the country replaces oil
```
### [`process.py`](process.py)
This script is designed to resolve the text mismatch between the timed `.srt` files and the "gold" standard `.txt` transcripts. It takes the timestamps from the `.srt` files and correctly aligns them with the gold transcript text.
1.  It reads the text from both the `.srt` (hypothesis) and `.txt` (reference) files.
2.  Due to the significant differences between the two texts, it uses a Levenshtein distance-based dynamic programming algorithm to find the optimal token-by-token alignment.
3.  It then calculates the start and end character offsets of the aligned gold text segment that corresponds to each original subtitle entry.
4.  The final output is a `.tsv` file containing these character offsets, which precisely maps the SRT timings to spans within the gold standard transcript. This approach also effectively "hides" the proprietary source text in the final public data, helping to address copyright concerns.
5.  The script includes an optional `--html` flag that, when used, will also generate a detailed, human-readable HTML report visualizing the alignment for debugging and analysis.

### `golds` data
`.tsv` file  
* Fields:
    * `start` - start time of the subtitle entry.
    * `end` - end time of the subtitle entry.
    * `alignment-start` - The start character offset of the aligned segment in the gold transcript file.
    * `alignment-end` - The end character offset of the aligned segment in the gold transcript file.

* Example:
```
$ head -4 cpb-aacip-507-1v5bc3tf81.tsv
index	start	end	content
1	00:02:05.570	00:02:08.570	Good evening. I'm Jim Lehrer. On the NewsHour tonight coverage
2	00:02:08.580	00:02:12.390	of the Salt Lake City Olympics investigation, some perspective on
3	00:02:12.400	00:02:16.540	this year of devastating storms and other natural disasters, a
```
_The golds data conforms to fieldname and time conventions._  
