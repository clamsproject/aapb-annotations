# NewsHour Transcript Sync

## Project Overview
To annotate the start times/end times of a written text transcript of audible speech within videos.  
In other words, providing the timestamps for speech recognition in closed captioning.  

### Specs
* Annotation Project Name - `newshour-transcript-sync`
* Annotator Demographics
    * Number of annotators - 1
    * Occupational description - College student
    * Age range - 20s
    * Education - College
* Annotation Environment Information
    * Name - CADET
    * Version - Unknown
    * Tool documentation - (see below tool installation)
* Project Changes
    * Number of batches - 1
    * Other version control information - None  

## Tool Installation: CADET
CADET - Caption and Description Editing Tool  
[CADET and Installation Instructions](https://www.wgbh.org/foundation/services/ncam/cadet)   
There are three basic ingredients to working with CADET. A video, its transcript, and the application itself.  
You'll have to have downloaded all of these before you can properly begin.  
  
To import the transcript, go to _File_, then _Import..._.  
Doing so will open up a window with the heading **Import Timed-Text File**.  
At the top left, you'll find a dropdown menu labeled _Import Type:_. You'll want to select Plain, otherwise, no transcripts will appear.  
Go to where you have the transcripts downloaded on your computer, and then select the appropriate one.  
  
Importing the video: Go to _File_, then _open Media..._.  
This will open up a window with the heading **Open Media File**.  
Then you will mostly follow the same process as importing a transcript, without the dropdown menu to select _Import Type:_.  
Go to where the videos are downloaded on your computer, then select the video.  
With both video and transcript downloaded, you can now do the closed captioning.  

## Annotation Guidelines
Transcribing/Closed Captioning  
> [!Note]  
> This `readme.md` is the guideline for this project.  
### Preparation
INPUT - Videos with audible speech to be annotated and transcripts matching those videos.  
  
First, break the transcript up into groups of 10 words. For every 10th word spoken, make a new line.  
* If a word is included within the transcript but isn't spoken in the video, delete it and don't count it when making new lines.  
* Closed Captioning exists to relay what is being spoken, so headers and descriptors in brackets aren't needed.  
Headers and descriptors in brackets include script instructions, stage directions, who is speaking, usually denoted by capital text or text in brackets eg.  (`JIM LEHRER`: I am saying something. `[MUSIC PLAYS]`)     
Once the transcript has been split into groups of 10 words, you can begin assigning those words to the appropriate times in the video.  
  
Second, youâ€™re going to want to go to _Player_, then hit _Play slower_ twice. This will play the video at .5x normal speed.  
### What to Annotate
Timestamping
* `text` - the text of the transcription is broken into pieces and entered into the annotation file from doing the above. 

* `start` (time) - When you hear the first word of the first line, hit the control and comma keys at the same time. (Different computers may have different hotkeys.)  
If you go to _Events_, the proper shortcut will be next to the option that says _Insert start time_.  
  
When you hear the first word of the second line, do the same thing, and so on and so forth.  
As you continue to do this, you'll go further down the transcript until you've gone through the whole thing.  
This can be a very long and tedious process, but it demands your complete attention for proper timing.  
That's also why you want the video at .5x normal speed.  

(Annotator:) I would go through videos with one finger over control and the other on comma.
This was probably the most demanding thing that I did during my time at WGBH. If you have any further questions, notify me (Gabriel) on Slack. 

* `end`(time) - end times were automatically generated by the CADET tool as 10ms prior to the next start time. When this project was done, end times were not annotated manually. 
Instead the decision was made to simply have the annotation of the time duration go until the next start time comes in. It is unvalidated what happened at the end of the transcript. 
### How to Annotate It
(See above)

### Decisions, Differentiation, and Precision Level during Annotation
* **`start` (time)** - In  Nov 01, 2022 (221101) when this first annotation was done, only start times were considered and annotated.  
* **`end` (time)** -  were automatically generated by the CADET tool as 10ms prior to the next start time.  

* **Long Pauses** - This means that if there was a long pause, the duration will continue to span until the next start-of-10-words was annotated.
Eg. in raw file [cpb-aacip-507-1v5bc3tf81.srt](https://github.com/clamsproject/aapb-annotations/blob/main/newshour-transcript-sync/221101-aapb-collaboration-21/cpb-aacip-507-1v5bc3tf81.srt), annotation #6 has a duration of 40s instead of the usual 4-5s.  

* **Time Format Unknown** - The time format in the raw files is likely milliseconds.
  
* **Text Mismatch to tool's given transcript** - As noted above, the transcript was edited and changed to remove non-spoken material. 
At the same time, it is possible that the text was also transformed by conjoining the spoken lines of different people. 
Further investigation is required. The issue is [mismatch of given ML transcript to gold annotation](https://github.com/clamsproject/aapb-annotations/issues/5).  
  
* **Other Difficulties and Interjections** - Closed Captioning/Subtitling of live interviews is difficult due to messy original data such as people speaking over each other. 
Here, "what the best subtitle would be" was used to guide annotator intuition.  
In cases where people spoke over each other (continuing=1st, interjecting=2nd), the given transcript often included what the interjector said after the transcription of what the continuing person said. 
The start time of the interjection is set at the moment when the first word of the interjection text is, and the continuing phrase from the previous line is annotated with an earlier end time (stopping where the interjection occurs).  
There are possible cases of multiple interjections, complete talking-over of people, pure unintelligible garble. The annotation of start times was done to the best of a reasonable ability.  
There is also little that could be done if the transcript did not confirm to the audio, and annotation would have to continue generally as best as possible with the mismatch as correcting it likely led to a slippery slope of revisions and decisions.  
  
* **Margin of Error** - Exact measurements of margin of error of when start times are clocked is not known, nor how much interjection or garble there was.  
However, with the above method of slowing down the audio to 50% speed and careful annotation, general beginning times should be as close as possible.
The datasets in this project are unvalidated for errors and precision. The assumption/hope is that the time error is approximately similar as described in the [repository_level_conventions.md](https://github.com/clamsproject/aapb-annotations/blob/main/repository_level_conventions.md).

## Data Format and `process.py`
### `raw` data
`.srt` file of lines of labels. Info about the [file format](https://mailchimp.com/resources/what-is-an-srt-file/#:~:text=An%20SRT%20file%2C%20or%20SubRip,any%20video%20or%20audio%20content.) here.    
* Fields: 
    * `label_number` - entry number
    * `start` - the starting time of that line of text.
    * `end` - the ending time of that line of text.
    * `text` - the 10 tokens of the closed captioning that are spoken to be timed in the transcript.
* Format:
```
label_number  
start --> end
text
empty_line  
```
* Example:
```
1
00:02:42,210 --> 00:02:46,860
Good evening. Energy Secretary James Schlesinger today uttered the direst

2
00:02:46,870 --> 00:02:50,990
warnings about the economic future unless the country replaces oil
```
### [`process.py`](process.py)
This script processes the `.srt` raw files into the `.tsv` gold files, and organizes the output `.tsv` files into subdirectories based on their batchnames.  
1. Uses `pysrt` to open `.srt` file and iterates through the subtitle entries in the file.  
2. For each subtitle entry, it extracts the index, start time, end time, and content of the subtitle. 
The start and end times are converted from the original format (with commas as decimal separators) to a format with periods as decimal separators. 
Any newline characters in the content are replaced with spaces.  
3. The extracted information is then written to the output .tsv file in a tab-separated format with headers for "index," "starts," "ends," and "content".


### `golds` data
`.tsv` file  
* Fields:
    * `index` - entry number 
    * `starts` - start time. 
    * `ends` - end time.
    * `content` - text of transcription.
_TODO: (discrepancy) note that the gold file format should conform to repo conventions to use `start` and `end` as a timeframe duration column header. 
The use of `content` is possibly ok, as `text` as a standardized format is still under discussion and may be too general of a term since each project is seeking different kinds of texts._  
* Example:
```
$ head -4 cpb-aacip-507-1v5bc3tf81.tsv
index	starts	ends	content
1	00:02:05.570	00:02:08.570	Good evening. I'm Jim Lehrer. On the NewsHour tonight coverage
2	00:02:08.580	00:02:12.390	of the Salt Lake City Olympics investigation, some perspective on
3	00:02:12.400	00:02:16.540	this year of devastating storms and other natural disasters, a
```
Note, the time format does conform to the ISO standard of hh:mm:ss.mmm.  
