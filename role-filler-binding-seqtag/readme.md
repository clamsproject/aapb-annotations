# Role-Filler Binding: Sequence Tagging

## Project Overview
This annotation effort uses LLM prompting to generate silver-standard annotations for training
the RFB model. All annotations were inspected and adjudicated by human annotators to ensure reasonable quality.

### Specs
* Annotation Project Name - `role-filler-binding-seqtag`
* Annotator Demographics
    * Number of annotators - Two
    * Occupational description - Graduate students
    * Age range - 20s
    * Education - Master's education
* Annotation Environment Information
    * Name - Streamlit app
    * Version - n/a
    * Tool documentation - (see below tool installation)
* Project Changes
    * Number of batches - 1
    * Other version control information - None

## Tool Installation

The code and instructions for using the Streamlit annotation environments can be found in the [`aapb-annenv-role-filler-binder`](https://github.com/clamsproject/aapb-annenv-role-filler-binder) repository, under the directory `llm-silver-anno`.
  
## Annotation Guidelines
Due to the ad hoc nature of the project, formal annotation guidelines were not prepared in advance. 

Three custom annotation environments were created using Streamlit for different phases of the task.

### Phase One
The first environment consisted of an interface displaying:
* a still frame from a video displaying text
* the docTR OCR results from that image
* the predicted SWT label (credits or chyron)
* the confidence score of that label

The main goal of this phase was data curation. For each instance, human annotators could choose from 4 actions:
(i) Accept the OCR results, (ii) Mark the entire OCR sequence as garbage (all tokens to be labeled as "O" by the model), (iii) swap the
scene label, (iv) Delete the instance from the corpus. The third option was chosen if the image was correctly labeled, 
but the OCR was too poor to analyze, while the fourth option was chosen in cases where the image did not actually
contain credits or a chyron.

### Phase Two

The second environment was created for human adjudication, and the goal of this task was to resolve disagreements between the two annotators during the first phase. 
The environment once again displayed the images and OCR results for each item in the corpus, but this time it also displayed the initial choice made by each annotator on either side of the image
(one on the left, and one on the right). Adjudication was conducted jointly by both annotators, who discussed 
each image and decided to either "Keep Left" or "Keep Right" (the initial choice made on the left or right).

### Phase Three

The third environment, called "Claude Adjudicator," consisted of:
* the video frame
* the docTR OCR text
* the *LLM-annotated* OCR text
* a preview of the resulting RFB-parsed text for the image (in JSON format).

Here the objective was to manually inspect and correct the annotations generated by the LLM. For each instance, human annotators could
either choose to reject it, which deleted the item from the dataset, or accept it, with the option to correct the text annotations
produced by the LLM. Deletion was used to filter out any data items that were erroneously retained in previous phases. 
This included 'null-role' cases, which are unable to be processed by the heuristic-based RFB parser.

### Preparation
All videos in the `aapb-annotation-44` batch were processed upstream and had metadata extracted via a series of CLAMS apps: 
SWT-detection, Simple-Timepoints-Stitcher, and docTR-wrapper. DocTR only processed scenes containing credits or chyrons
for this round of annotation. Each OCR result from docTR was lightly de-noised in the pre-processing --
all stray punctuation and non-alpha lines were removed. Note that no OCR spell-check correction was performed. Next, the
time point associated with each text document was leveraged to obtain the corresponding SWT label and its confidence score.
Thus prior to any annotation, the initial CSV file contained the GUID, time point, scene label, confidence score, and 
ocr text for each item in the corpus.

### Prompting the LLM

Claude 3 Haiku by Anthropic was used to generate labels for each OCR token sequence in the dataset.
For each token, the model was instructed to append a BIO tag (with "R" for Role; "F" for Filler), and an index to bind each filler to its corresponding role.
Labels were appended to tokens using an arbitrary `@` delimiter, resulting in a format that looks like `token@tag:index`.
Two versions of a prompt were written for the Haiku model (one for annotating credits, the other for chyrons). The prompt templates
used are available [here](prompt_templates.txt).

### What to Annotate

#### Phase 1 - Data Curation
* `Continue` - Accept the OCR result and the SWT label.
* `Reject OCR` - Flag the OCR text as containing neither role nor filler (the text is too noisy), accept the SWT label.
* `Swap` - Swap the SWT label to credits/chyron
* `Delete` - Delete the instance from the dataset / neither credit nor chyron.

#### Phase 2 - Human Adjudicator
* `Keep Left` - Keep the Left Annotator's choice.
* `Keep Right` - Keep the Right Annotator's choice.

#### Phase 3 - Claude Adjudicator
* `üëç` - Accept Haiku's annotation. The user may optionally correct the annotation.
* `üëé`- Reject and delete the annotation.

### How to Annotate It
Button press, or by using specified key-binds mapped to the buttons.

### Decisions, Differentiation, and Precision Level during Annotation
- (Phase One) **Criteria for noisy OCR** - The RFB model should return an empty dictionary in cases where the OCR results are unintelligible. OCR text that does not contain any tokens resembling names (i.e. a clear first and last name) or job titles should have all of its tokens labeled as "O". Annotators used their discretion for this, and any disagreements were resolved during adjudication.
- (Phase Three) **LLM Hallucinations** - When inspecting LLM-generated annotations, human annotators must ensure that Haiku does not correct OCR misspellings in its outputs, and that the number of tokens in the original sequence is equal to the number of generated labels. For example, Haiku may attempt to merge or split a "bad" OCR token (e.g. "ExecutiveProducer", "Di rector"). However, this results in the number of input tokens and the number of generated labels being unequal, so the original OCR errors must be retained for training the RFB model.

## Data Format and `process.py`

### `raw` data
`.CSV` file - explanation.
* Fields:
    * `guid` - Identifier for the video
    * `timePoint` - The time point in the video where the text was captured.
    * `scene_label` - The predicted SWT label
    * `cleaned_text` - The pre-processed OCR text. This was the focus of annotation.
    * `silver_standard_annotation` - The annotation generated by the LLM.
* Example:

| guid                     | timePoint | scene_label | cleaned_text                         | silver_standard_annotation                                         |
|--------------------------|-----------|-------------|--------------------------------------|--------------------------------------------------------------------|
| cpb-aacip-507-154dn40c26 | 3218218   | chyron      | DEBBE HALL Farmer                    | DEBBE@BF:1 HALL@IF:1 Farmer@BR:1                                   |
| cpb-aacip-526-dj58c9s78v | 3471471   | credits     | DIRECTED BY Barry Stoner Fred Wessel | DIRECTED@BR:1 BY@IR:1 Barry@BF:1 Stoner@IF:1 Fred@BF:1 Wessel@IF:1 |

### [`process.py`](process.py)
Converts the data set annotated by the Claude 3 Haiku LLM into a format ready for training the BERT model to perform token classification. To prepare the model inputs, 
the script prepends the corresponding scene label to each ocr text sequence before splitting the sequence into a list of tokens.
To prepare the labels, for each ocr text sequence annotated by Haiku (format: `token@tag:index`), all the tags are
extracted and compiled into a list the same length as the token sequence. The end result is a dataframe consisting of just the text tokens
and their corresponding tags. Finally, this dataframe is shuffled and partitioned into train, validation, and test splits
(8:1:1 ratio) in JSON format.

### `golds` data
N/A -- This annotation project was not prepared on a media-file basis. Rather, since our annotation effort resulted in a
CSV containing one row per OCR result (and consequently, multiple rows per GUID), these items were shuffled randomly, irrespective of the GUID,
before being split into partitions for training and evaluating the model. Generating a separate CSV for each GUID would 
result in a format that does not replicate the procedure used for training the model.

## See also
[Role-Filler-Binding](https://github.com/clamsproject/aapb-annotations/tree/main/role-filler-binding): 
The human-annotated counterpart of this project. This effort includes manual OCR correction, and is developed for evaluation purposes.
