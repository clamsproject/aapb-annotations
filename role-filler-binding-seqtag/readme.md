# Role-Filler Binding: Sequence Tagging

## Project Overview
This annotation effort uses LLM prompting to generate silver-standard annotations for training
the RFB model. All annotations were adjudicated by human annotators to ensure reasonable quality.

### Specs
* Annotation Project Name - `role-filler-binding-seqtag`
* Annotator Demographics
    * Number of annotators - Two
    * Occupational description - Graduate students
    * Age range - 20s
    * Education - Master's education
* Annotation Environment Information
    * Name - Streamlit
    * Version - n/a
    * Tool documentation - (see below tool installation)
* Project Changes
    * Number of batches - 1
    * Other version control information - None

## Tool Installation: Name of Tool
(link for source code, if applicable)  
(link for documentation, if applicable)  
  
## Annotation Guidelines
Due to the ad hoc nature of the project, formal annotation guidelines were not prepared in advance. 

Three annotation environments were created using Streamlit. 
The first environment consisted of an interface displaying:
* a still frame from a video displaying text
* the docTR OCR results from that image
* the predicted SWT label (credits or chyron)
* the confidence score of that label

The main goal of this stage was data curation. For each instance, human annotators could choose from 4 actions:
(i) Accept the OCR results, (ii) Mark the entire OCR sequence as garbage (to be labeled as "O" by the model), (iii) swap the
scene label, (iv) Delete the instance from the corpus. The third option was chosen if the image was correctly labeled, 
but the OCR was too poor to analyze, while the fourth option was chosen in cases where the image did not actually
contain credits or a chyron.

The second environment served as a human adjudicator, and the goal of this task was to resolve disagreements in judgements 
on the first stage between the two annotators. The environment once again displayed the images and OCR results for each item
 in the corpus, but this time it also displayed the initial choice made by each annotator on either side of the image
(one on the left, and one on the right). This phase of annotation was conducted jointly by both annotators, who discussed 
each image and decided to either "Keep Left" or "Keep Right" (the choice made on the left or right).

The third environment, called "Claude Adjudicator," consisted of:
* the video frame
* the corresponding OCR text
* the *LLM-annotated* OCR text
* a preview of the resulting RFB-parsed text for the image (in JSON format).

Here the objective was to manually inspect and correct the annotations generated by the LLM. For each instance, human annotators could
either choose to reject it, which deleted the item from the dataset, or accept it, with the option to correct the text annotations
produced by the LLM. Deletion was used to filter out any data items that were erroneously retained in previous phases. 
This included 'null-role' cases, which are incompatible with the heuristic-based RFB parser.

### Preparation
All videos in the aapb-annotation-44 batch were processed upstream and had metadata extracted via a series of CLAMS apps: 
SWT-detection, Simple-Timepoints-Stitcher, and docTR-wrapper. DocTR only processed scenes containing credits or chyrons
for this round of annotation. Each OCR result from docTR was lightly de-noised in the pre-processing --
all stray punctuation and non-alpha lines were removed. Note that no OCR spell-check correction was performed. Next, the
time point associated with each text document was leveraged to obtain the corresponding SWT label and its confidence score.
Thus prior to any annotation, the initial CSV file contained the GUID, time point, scene label, confidence score, and 
ocr text for each item in the corpus.

### Prompting the LLM

Two versions of a prompt were written for the Haiku model (one for annotating credits, the other for chyrons). The templates
used are available [here]().

### What to Annotate

#### Phase 1 - Data Curation
* `Continue` - Accept the OCR result and the SWT label.
* `Reject` - Flag the OCR text as containing neither role nor filler (the text is too noisy), accept the SWT label.
* `Swap` - Swap the SWT label to credits/chyron
* `Delete` - Delete the instance from the dataset / neither credit nor chyron.

#### Phase 2 - Human Adjudicator
* `Keep Left` - Keep the Left Annotator's choice.
* `Keep Right` - Keep the Right Annotator's choice.

#### Phase 3 - Claude Adjudicator
* `üëç` - Accept Haiku's annotation. The user may optionally correct the text.
* `üëé`- Reject and delete the annotation.

### How to Annotate It
Button press, or by using the specified key-binds mapped to the buttons.

### Decisions, Differentiation, and Precision Level during Annotation
- **Problem  Name** - Information about Decisions that must be made during Annotation, for instance, Differentiation between two slate types. 
#### Data Quality Efforts (or other subheaders, optional)
- **Problem  Name** - Information on accuracy, precision, error that may be present in the dataset.

## Data Format and `process.py`

### `raw` data
`.CSV` file - explanation.
* Fields: (possibly referring back to the "what to annotate" section above)
    * `guid` - Identifier for the video
    * `timePoint` - The time point in the video where the text was captured.
    * `scene_label` - The predicted SWT label
    * `cleaned_text` - The pre-processed OCR text. This was the focus of annotation.
* Example:

| guid                     | timePoint | scene_label | cleaned_text |
|--------------------------|-----------|-------------|--------------|
| cpb-aacip-526-dj58c9s78v | 1187187   | chyron      | Glen Miller  |

### [`process.py`](process.py)
Converts the data set annotated by the Claude 3 Haiku LLM into a format ready for training the BERT model. To prepare the model inputs, 
the script prepends the corresponding scene label to each ocr text sequence before splitting the sequence into a list of tokens.
To prepare the labels, for each ocr text sequence annotated by Haiku (format: `token@tag:index`), all the tags are
extracted and compiled into a list the same length as the text sequence. The end result is a dataframe consisting of just the text tokens
and their corresponding tags. Finally, this dataframe is shuffled and partitioned into train, validation, and test splits
(8:1:1 ratio) in JSON format.

### `golds` data
N/A -- This annotation project was not prepared on a media-file basis. Rather, since our annotation effort resulted in a
CSV containing one row per OCR result (and consequently, multiple rows per GUID), these items were shuffled randomly, irrespective of the GUID,
before being split into partitions for training and evaluating the model. Generating a separate CSV for each GUID would 
result in a format that does not replicate the procedure used for training the model.

## See also (optional, if applicable)
[Role-Filler-Binding](https://github.com/clamsproject/aapb-annotations/tree/main/role-filler-binding): 
The human-annotated counterpart of this project. This effort includes manual OCR correction, and is developed for evaluation purposes.
